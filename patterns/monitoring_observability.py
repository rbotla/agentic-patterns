"""
Agent Monitoring & Observability Pattern Implementation
Comprehensive monitoring, metrics collection, tracing, and observability
for multi-agent systems with performance analytics and anomaly detection
"""

import autogen
from typing import Dict, Any, Optional, List, Callable, Union
import logging
from utils.logging_utils import setup_logging
import time
import json
import threading
from dataclasses import dataclass, field, asdict
from enum import Enum
from collections import defaultdict, deque
import psutil
import traceback
from contextlib import contextmanager
import numpy as np

logger = setup_logging(__name__)


class MetricType(Enum):
    """Types of metrics to collect"""
    COUNTER = "counter"           # Incrementing values
    GAUGE = "gauge"              # Current state values  
    HISTOGRAM = "histogram"       # Distribution of values
    TIMER = "timer"              # Duration measurements
    RATE = "rate"                # Events per time unit


class AlertSeverity(Enum):
    """Alert severity levels"""
    CRITICAL = "critical"
    WARNING = "warning"
    INFO = "info"
    DEBUG = "debug"


@dataclass
class Metric:
    """Individual metric data point"""
    name: str
    value: Union[int, float]
    metric_type: MetricType
    timestamp: float
    labels: Dict[str, str] = field(default_factory=dict)
    unit: Optional[str] = None
    
    def to_dict(self) -> Dict:
        """Convert to dictionary"""
        return asdict(self)


@dataclass
class Alert:
    """Alert generated by monitoring system"""
    id: str
    severity: AlertSeverity
    message: str
    source: str
    timestamp: float
    metadata: Dict[str, Any] = field(default_factory=dict)
    resolved: bool = False
    resolved_at: Optional[float] = None
    
    def resolve(self):
        """Mark alert as resolved"""
        self.resolved = True
        self.resolved_at = time.time()


@dataclass
class TraceSpan:
    """Distributed tracing span"""
    trace_id: str
    span_id: str
    parent_span_id: Optional[str]
    operation_name: str
    start_time: float
    end_time: Optional[float] = None
    duration: Optional[float] = None
    tags: Dict[str, Any] = field(default_factory=dict)
    logs: List[Dict[str, Any]] = field(default_factory=list)
    status: str = "active"
    
    def finish(self):
        """Finish the span"""
        self.end_time = time.time()
        self.duration = self.end_time - self.start_time
        self.status = "completed"
    
    def add_tag(self, key: str, value: Any):
        """Add a tag to the span"""
        self.tags[key] = value
    
    def log(self, event: str, payload: Dict[str, Any] = None):
        """Add a log entry to the span"""
        self.logs.append({
            "timestamp": time.time(),
            "event": event,
            "payload": payload or {}
        })


class MetricsCollector:
    """Collects and aggregates metrics"""
    
    def __init__(self):
        self.metrics: Dict[str, List[Metric]] = defaultdict(list)
        self.lock = threading.Lock()
        self.aggregated_metrics = {}
        self._start_time = time.time()
    
    def record_counter(self, name: str, value: int = 1, labels: Dict[str, str] = None):
        """Record a counter metric"""
        metric = Metric(
            name=name,
            value=value,
            metric_type=MetricType.COUNTER,
            timestamp=time.time(),
            labels=labels or {}
        )
        
        with self.lock:
            self.metrics[name].append(metric)
    
    def record_gauge(self, name: str, value: float, labels: Dict[str, str] = None):
        """Record a gauge metric"""
        metric = Metric(
            name=name,
            value=value,
            metric_type=MetricType.GAUGE,
            timestamp=time.time(),
            labels=labels or {}
        )
        
        with self.lock:
            self.metrics[name].append(metric)
    
    def record_histogram(self, name: str, value: float, labels: Dict[str, str] = None):
        """Record a histogram metric"""
        metric = Metric(
            name=name,
            value=value,
            metric_type=MetricType.HISTOGRAM,
            timestamp=time.time(),
            labels=labels or {}
        )
        
        with self.lock:
            self.metrics[name].append(metric)
    
    @contextmanager
    def timer(self, name: str, labels: Dict[str, str] = None):
        """Context manager for timing operations"""
        start_time = time.time()
        try:
            yield
        finally:
            duration = time.time() - start_time
            self.record_histogram(f"{name}_duration", duration, labels)
    
    def get_metrics_summary(self, window_seconds: int = 300) -> Dict[str, Any]:
        """Get summary of metrics within time window"""
        cutoff_time = time.time() - window_seconds
        summary = {}
        
        with self.lock:
            for metric_name, metric_list in self.metrics.items():
                recent_metrics = [m for m in metric_list if m.timestamp >= cutoff_time]
                
                if not recent_metrics:
                    continue
                
                values = [m.value for m in recent_metrics]
                
                summary[metric_name] = {
                    "count": len(recent_metrics),
                    "sum": sum(values),
                    "avg": np.mean(values),
                    "min": min(values),
                    "max": max(values),
                    "std": np.std(values),
                    "last_value": values[-1] if values else 0,
                    "rate_per_second": len(recent_metrics) / window_seconds
                }
        
        return summary
    
    def cleanup_old_metrics(self, max_age_seconds: int = 3600):
        """Remove metrics older than max_age_seconds"""
        cutoff_time = time.time() - max_age_seconds
        
        with self.lock:
            for metric_name in list(self.metrics.keys()):
                self.metrics[metric_name] = [
                    m for m in self.metrics[metric_name] 
                    if m.timestamp >= cutoff_time
                ]
                
                if not self.metrics[metric_name]:
                    del self.metrics[metric_name]


class DistributedTracer:
    """Distributed tracing for multi-agent operations"""
    
    def __init__(self):
        self.active_traces: Dict[str, TraceSpan] = {}
        self.completed_traces: List[TraceSpan] = []
        self.lock = threading.Lock()
        self._trace_counter = 0
    
    def start_trace(
        self,
        operation_name: str,
        parent_span_id: Optional[str] = None,
        tags: Dict[str, Any] = None
    ) -> TraceSpan:
        """Start a new trace span"""
        with self.lock:
            self._trace_counter += 1
            
            span = TraceSpan(
                trace_id=f"trace_{self._trace_counter}",
                span_id=f"span_{self._trace_counter}",
                parent_span_id=parent_span_id,
                operation_name=operation_name,
                start_time=time.time(),
                tags=tags or {}
            )
            
            self.active_traces[span.span_id] = span
            return span
    
    def finish_trace(self, span_id: str):
        """Finish a trace span"""
        with self.lock:
            if span_id in self.active_traces:
                span = self.active_traces.pop(span_id)
                span.finish()
                self.completed_traces.append(span)
                return span
        return None
    
    @contextmanager
    def trace_operation(
        self,
        operation_name: str,
        tags: Dict[str, Any] = None
    ):
        """Context manager for tracing operations"""
        span = self.start_trace(operation_name, tags=tags)
        try:
            yield span
        except Exception as e:
            span.add_tag("error", True)
            span.add_tag("error_message", str(e))
            span.log("error", {"exception": str(e), "traceback": traceback.format_exc()})
            raise
        finally:
            self.finish_trace(span.span_id)
    
    def get_trace_tree(self, trace_id: str) -> List[TraceSpan]:
        """Get all spans for a trace as a tree structure"""
        spans = [s for s in self.completed_traces if s.trace_id == trace_id]
        spans.extend([s for s in self.active_traces.values() if s.trace_id == trace_id])
        
        # Sort by start time for tree construction
        spans.sort(key=lambda s: s.start_time)
        return spans
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Get performance summary from traces"""
        if not self.completed_traces:
            return {"message": "No completed traces"}
        
        operations = defaultdict(list)
        for span in self.completed_traces:
            if span.duration:
                operations[span.operation_name].append(span.duration)
        
        summary = {}
        for op_name, durations in operations.items():
            summary[op_name] = {
                "count": len(durations),
                "avg_duration": np.mean(durations),
                "min_duration": min(durations),
                "max_duration": max(durations),
                "p95_duration": np.percentile(durations, 95),
                "p99_duration": np.percentile(durations, 99)
            }
        
        return summary


class AlertManager:
    """Manages alerts and notifications"""
    
    def __init__(self):
        self.alerts: List[Alert] = []
        self.alert_rules: List[Callable] = []
        self.alert_counter = 0
        self.lock = threading.Lock()
        self.notification_handlers = []
    
    def add_alert_rule(self, rule_func: Callable[[Dict], Optional[Alert]]):
        """Add an alert rule function"""
        self.alert_rules.append(rule_func)
    
    def add_notification_handler(self, handler: Callable[[Alert], None]):
        """Add a notification handler"""
        self.notification_handlers.append(handler)
    
    def check_alerts(self, metrics: Dict[str, Any]):
        """Check metrics against alert rules"""
        with self.lock:
            for rule in self.alert_rules:
                try:
                    alert = rule(metrics)
                    if alert:
                        self._trigger_alert(alert)
                except Exception as e:
                    logger.error(f"Error in alert rule: {e}")
    
    def _trigger_alert(self, alert: Alert):
        """Trigger an alert and notify handlers"""
        self.alerts.append(alert)
        logger.warning(f"ALERT [{alert.severity.value}]: {alert.message}")
        
        for handler in self.notification_handlers:
            try:
                handler(alert)
            except Exception as e:
                logger.error(f"Error in notification handler: {e}")
    
    def resolve_alert(self, alert_id: str) -> bool:
        """Resolve an alert"""
        with self.lock:
            for alert in self.alerts:
                if alert.id == alert_id and not alert.resolved:
                    alert.resolve()
                    logger.info(f"Alert resolved: {alert_id}")
                    return True
        return False
    
    def get_active_alerts(self) -> List[Alert]:
        """Get all unresolved alerts"""
        return [a for a in self.alerts if not a.resolved]
    
    def create_alert(
        self,
        message: str,
        severity: AlertSeverity = AlertSeverity.WARNING,
        source: str = "system",
        metadata: Dict[str, Any] = None
    ) -> Alert:
        """Create a new alert"""
        with self.lock:
            self.alert_counter += 1
            alert = Alert(
                id=f"alert_{self.alert_counter}",
                severity=severity,
                message=message,
                source=source,
                timestamp=time.time(),
                metadata=metadata or {}
            )
            return alert


class SystemMonitor:
    """Monitors system resources"""
    
    def __init__(self, metrics_collector: MetricsCollector):
        self.metrics_collector = metrics_collector
        self.monitoring = False
        self.monitor_thread = None
    
    def start_monitoring(self, interval: float = 5.0):
        """Start system monitoring"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            args=(interval,),
            daemon=True
        )
        self.monitor_thread.start()
        logger.info("System monitoring started")
    
    def stop_monitoring(self):
        """Stop system monitoring"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        logger.info("System monitoring stopped")
    
    def _monitor_loop(self, interval: float):
        """Main monitoring loop"""
        while self.monitoring:
            try:
                # CPU metrics
                cpu_percent = psutil.cpu_percent(interval=1)
                self.metrics_collector.record_gauge("system_cpu_percent", cpu_percent)
                
                # Memory metrics
                memory = psutil.virtual_memory()
                self.metrics_collector.record_gauge("system_memory_percent", memory.percent)
                self.metrics_collector.record_gauge("system_memory_available", memory.available)
                
                # Disk metrics
                disk = psutil.disk_usage('/')
                self.metrics_collector.record_gauge("system_disk_percent", 
                                                  (disk.used / disk.total) * 100)
                
                # Process metrics
                process = psutil.Process()
                self.metrics_collector.record_gauge("process_memory_rss", process.memory_info().rss)
                self.metrics_collector.record_gauge("process_cpu_percent", process.cpu_percent())
                
                time.sleep(interval)
                
            except Exception as e:
                logger.error(f"Error in system monitoring: {e}")
                time.sleep(interval)


class MonitoredAgent:
    """Agent wrapper with comprehensive monitoring"""
    
    def __init__(
        self,
        name: str,
        agent: autogen.ConversableAgent,
        metrics_collector: MetricsCollector,
        tracer: DistributedTracer
    ):
        self.name = name
        self.agent = agent
        self.metrics_collector = metrics_collector
        self.tracer = tracer
        
        # Override agent methods with monitoring
        self._wrap_agent_methods()
        
        # Agent-specific metrics
        self.conversation_count = 0
        self.error_count = 0
        self.total_response_time = 0
    
    def _wrap_agent_methods(self):
        """Wrap agent methods with monitoring"""
        original_generate_reply = self.agent.generate_reply
        
        def monitored_generate_reply(*args, **kwargs):
            with self.tracer.trace_operation(f"{self.name}_generate_reply") as span:
                span.add_tag("agent_name", self.name)
                span.add_tag("args_count", len(args))
                
                start_time = time.time()
                
                try:
                    # Record request
                    self.metrics_collector.record_counter(
                        "agent_requests_total",
                        labels={"agent": self.name}
                    )
                    
                    result = original_generate_reply(*args, **kwargs)
                    
                    # Record success
                    self.metrics_collector.record_counter(
                        "agent_requests_success",
                        labels={"agent": self.name}
                    )
                    
                    self.conversation_count += 1
                    span.add_tag("success", True)
                    
                    return result
                    
                except Exception as e:
                    # Record error
                    self.metrics_collector.record_counter(
                        "agent_requests_error",
                        labels={"agent": self.name, "error_type": type(e).__name__}
                    )
                    
                    self.error_count += 1
                    span.add_tag("error", True)
                    span.add_tag("error_type", type(e).__name__)
                    
                    raise
                    
                finally:
                    # Record response time
                    response_time = time.time() - start_time
                    self.total_response_time += response_time
                    
                    self.metrics_collector.record_histogram(
                        "agent_response_time",
                        response_time,
                        labels={"agent": self.name}
                    )
                    
                    span.add_tag("response_time", response_time)
        
        self.agent.generate_reply = monitored_generate_reply
    
    def get_agent_stats(self) -> Dict[str, Any]:
        """Get agent-specific statistics"""
        avg_response_time = (self.total_response_time / max(self.conversation_count, 1))
        
        return {
            "name": self.name,
            "conversation_count": self.conversation_count,
            "error_count": self.error_count,
            "error_rate": self.error_count / max(self.conversation_count, 1),
            "avg_response_time": avg_response_time,
            "total_response_time": self.total_response_time
        }


class ObservabilityDashboard:
    """Central observability dashboard"""
    
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.tracer = DistributedTracer()
        self.alert_manager = AlertManager()
        self.system_monitor = SystemMonitor(self.metrics_collector)
        self.monitored_agents: Dict[str, MonitoredAgent] = {}
        
        # Setup default alert rules
        self._setup_default_alerts()
    
    def _setup_default_alerts(self):
        """Setup default alerting rules"""
        
        def high_cpu_alert(metrics: Dict) -> Optional[Alert]:
            cpu_metric = metrics.get("system_cpu_percent", {})
            if cpu_metric.get("last_value", 0) > 80:
                return self.alert_manager.create_alert(
                    f"High CPU usage: {cpu_metric['last_value']:.1f}%",
                    AlertSeverity.WARNING,
                    "system_monitor"
                )
            return None
        
        def high_error_rate_alert(metrics: Dict) -> Optional[Alert]:
            error_metric = metrics.get("agent_requests_error", {})
            success_metric = metrics.get("agent_requests_success", {})
            
            if error_metric and success_metric:
                total = error_metric.get("sum", 0) + success_metric.get("sum", 0)
                if total > 10:  # Only alert if we have significant traffic
                    error_rate = error_metric.get("sum", 0) / total
                    if error_rate > 0.1:  # 10% error rate
                        return self.alert_manager.create_alert(
                            f"High error rate: {error_rate:.1%}",
                            AlertSeverity.CRITICAL,
                            "agent_monitor"
                        )
            return None
        
        self.alert_manager.add_alert_rule(high_cpu_alert)
        self.alert_manager.add_alert_rule(high_error_rate_alert)
    
    def register_agent(self, agent: autogen.ConversableAgent) -> MonitoredAgent:
        """Register an agent for monitoring"""
        monitored_agent = MonitoredAgent(
            agent.name,
            agent,
            self.metrics_collector,
            self.tracer
        )
        
        self.monitored_agents[agent.name] = monitored_agent
        logger.info(f"Registered agent for monitoring: {agent.name}")
        
        return monitored_agent
    
    def start_monitoring(self):
        """Start all monitoring systems"""
        self.system_monitor.start_monitoring()
    
    def stop_monitoring(self):
        """Stop all monitoring systems"""
        self.system_monitor.stop_monitoring()
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """Get comprehensive dashboard data"""
        metrics_summary = self.metrics_collector.get_metrics_summary()
        trace_summary = self.tracer.get_performance_summary()
        active_alerts = self.alert_manager.get_active_alerts()
        
        # Agent statistics
        agent_stats = {}
        for name, monitored_agent in self.monitored_agents.items():
            agent_stats[name] = monitored_agent.get_agent_stats()
        
        # Check alerts
        self.alert_manager.check_alerts(metrics_summary)
        
        return {
            "timestamp": time.time(),
            "metrics": metrics_summary,
            "traces": trace_summary,
            "agents": agent_stats,
            "active_alerts": [
                {
                    "id": a.id,
                    "severity": a.severity.value,
                    "message": a.message,
                    "source": a.source,
                    "timestamp": a.timestamp
                }
                for a in active_alerts
            ],
            "system_health": self._calculate_system_health(metrics_summary)
        }
    
    def _calculate_system_health(self, metrics: Dict) -> str:
        """Calculate overall system health score"""
        health_score = 100
        
        # CPU health
        cpu = metrics.get("system_cpu_percent", {}).get("last_value", 0)
        if cpu > 90:
            health_score -= 30
        elif cpu > 70:
            health_score -= 15
        
        # Memory health  
        memory = metrics.get("system_memory_percent", {}).get("last_value", 0)
        if memory > 90:
            health_score -= 20
        elif memory > 80:
            health_score -= 10
        
        # Error rate health
        error_metric = metrics.get("agent_requests_error", {})
        success_metric = metrics.get("agent_requests_success", {})
        
        if error_metric and success_metric:
            total = error_metric.get("sum", 0) + success_metric.get("sum", 0)
            if total > 0:
                error_rate = error_metric.get("sum", 0) / total
                if error_rate > 0.1:
                    health_score -= 25
                elif error_rate > 0.05:
                    health_score -= 10
        
        if health_score >= 90:
            return "excellent"
        elif health_score >= 70:
            return "good"
        elif health_score >= 50:
            return "fair"
        else:
            return "poor"


def run_monitoring_examples():
    """Demonstrate monitoring and observability patterns"""
    logger.info("Starting Monitoring & Observability Pattern Examples")
    
    logger.info("\n=== Basic Metrics Collection Example ===")
    
    # Create metrics collector
    metrics = MetricsCollector()
    
    # Record various metrics
    for i in range(10):
        metrics.record_counter("requests_total", labels={"method": "GET"})
        metrics.record_gauge("queue_size", i * 2.5)
        metrics.record_histogram("response_time", np.random.normal(0.1, 0.02))
        
        time.sleep(0.1)
    
    # Get summary
    summary = metrics.get_metrics_summary(window_seconds=10)
    logger.info(f"Metrics summary: {json.dumps(summary, indent=2, default=str)}")
    
    logger.info("\n=== Distributed Tracing Example ===")
    
    # Create tracer
    tracer = DistributedTracer()
    
    # Trace a complex operation
    with tracer.trace_operation("complex_workflow") as root_span:
        root_span.add_tag("workflow_id", "wf_001")
        
        # Simulate sub-operations
        with tracer.trace_operation("data_preprocessing") as prep_span:
            prep_span.add_tag("dataset_size", 1000)
            time.sleep(0.1)
            prep_span.log("preprocessing_complete", {"rows_processed": 1000})
        
        with tracer.trace_operation("model_training") as train_span:
            train_span.add_tag("model_type", "random_forest")
            time.sleep(0.2)
            train_span.log("training_complete", {"accuracy": 0.95})
        
        root_span.log("workflow_complete", {"total_duration": time.time() - root_span.start_time})
    
    # Get performance summary
    perf_summary = tracer.get_performance_summary()
    logger.info(f"Trace performance: {json.dumps(perf_summary, indent=2, default=str)}")
    
    logger.info("\n=== Alert Management Example ===")
    
    # Create alert manager
    alert_mgr = AlertManager()
    
    # Add notification handler
    def log_alert(alert: Alert):
        logger.warning(f"NOTIFICATION: {alert.severity.value} - {alert.message}")
    
    alert_mgr.add_notification_handler(log_alert)
    
    # Create test alerts
    critical_alert = alert_mgr.create_alert(
        "System memory usage critical: 95%",
        AlertSeverity.CRITICAL,
        "system_monitor",
        {"memory_percent": 95, "threshold": 90}
    )
    alert_mgr._trigger_alert(critical_alert)
    
    warning_alert = alert_mgr.create_alert(
        "High response latency detected",
        AlertSeverity.WARNING,
        "performance_monitor",
        {"avg_latency": 2.5, "threshold": 2.0}
    )
    alert_mgr._trigger_alert(warning_alert)
    
    logger.info(f"Active alerts: {len(alert_mgr.get_active_alerts())}")
    
    logger.info("\n=== Monitored Agent Example ===")
    
    # Create observability dashboard
    dashboard = ObservabilityDashboard()
    
    # Create and register agents
    test_agent = autogen.ConversableAgent(
        name="test_agent",
        llm_config=False,
        human_input_mode="NEVER"
    )
    
    # Simple reply function for testing
    test_agent.register_reply(
        trigger=lambda sender: True,
        reply_func=lambda **kwargs: (True, "Test response"),
        position=0
    )
    
    monitored_agent = dashboard.register_agent(test_agent)
    
    # Start monitoring
    dashboard.start_monitoring()
    
    # Simulate agent activity
    for i in range(5):
        try:
            response = test_agent.generate_reply(
                messages=[{"role": "user", "content": f"Test message {i}"}]
            )
            time.sleep(0.1)
        except Exception as e:
            logger.error(f"Agent error: {e}")
    
    # Get dashboard data
    dashboard_data = dashboard.get_dashboard_data()
    
    logger.info(f"System health: {dashboard_data['system_health']}")
    logger.info(f"Agent stats: {json.dumps(dashboard_data['agents'], indent=2)}")
    logger.info(f"Active alerts: {len(dashboard_data['active_alerts'])}")
    
    # Stop monitoring
    dashboard.stop_monitoring()
    
    logger.info("\n=== Performance Benchmarking Example ===")
    
    # Benchmark different operations
    operations = {
        "fast_operation": lambda: time.sleep(0.01),
        "medium_operation": lambda: time.sleep(0.1),
        "slow_operation": lambda: time.sleep(0.2)
    }
    
    benchmark_metrics = MetricsCollector()
    
    for op_name, op_func in operations.items():
        for _ in range(10):
            with benchmark_metrics.timer(f"benchmark_{op_name}"):
                op_func()
    
    benchmark_summary = benchmark_metrics.get_metrics_summary()
    
    logger.info("Performance Benchmark Results:")
    for metric_name, stats in benchmark_summary.items():
        if "duration" in metric_name:
            logger.info(f"  {metric_name}: avg={stats['avg']:.3f}s, "
                       f"min={stats['min']:.3f}s, max={stats['max']:.3f}s")
    
    logger.info("\nMonitoring & Observability Pattern Examples Complete")


if __name__ == "__main__":
    run_monitoring_examples()